{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Report\n",
    "\n",
    "\"What, if any, are the characteristics of a good boy?\" \n",
    "\n",
    "A poignant question with no easy answers. In our effort to answer this, we'll turn to the WeRateDogs twitter account, a world renowned resource for finding the goodest boys amongst us. However, before we can find the best doggos of them all (and they are all good dogs, Brent), we need to make that data viable for use internally. In order to do that, we'll proceed through the usual three step process for wrangling our data; gathering, assessing and cleaning. Our data will come from three sources: the WeRateDogs twitter archive, contained in the csv `twitter-archive-enhanced.csv`, the image predictions we can pull down remotely from the file `image-predictions.tsv`, and external data from the Twitter API. So let's go about gathering those three sources, and then we can assess their tidiness, cleanliness, and then get them sorted so we can appreciate every pupper they contain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Gathering Our Data\n",
    "#### File 1: The Twitter Archive\n",
    "\n",
    "\n",
    "This will actually turn out to be the easiest of the bunch, from a \"gathering\" standpoint. As the file was provided to us by Udacity, we can simply place it in our local directory, and import it directly into a Pandas dataframe, where it will be ready for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2356 entries, 0 to 2355\n",
      "Data columns (total 17 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   tweet_id                    2356 non-null   int64  \n",
      " 1   in_reply_to_status_id       78 non-null     float64\n",
      " 2   in_reply_to_user_id         78 non-null     float64\n",
      " 3   timestamp                   2356 non-null   object \n",
      " 4   source                      2356 non-null   object \n",
      " 5   text                        2356 non-null   object \n",
      " 6   retweeted_status_id         181 non-null    float64\n",
      " 7   retweeted_status_user_id    181 non-null    float64\n",
      " 8   retweeted_status_timestamp  181 non-null    object \n",
      " 9   expanded_urls               2297 non-null   object \n",
      " 10  rating_numerator            2356 non-null   int64  \n",
      " 11  rating_denominator          2356 non-null   int64  \n",
      " 12  name                        2356 non-null   object \n",
      " 13  doggo                       2356 non-null   object \n",
      " 14  floofer                     2356 non-null   object \n",
      " 15  pupper                      2356 non-null   object \n",
      " 16  puppo                       2356 non-null   object \n",
      "dtypes: float64(4), int64(3), object(10)\n",
      "memory usage: 313.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# import the we rate dogs twitter archive\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "archive = pd.read_csv('twitter-archive-enhanced.csv')\n",
    "archive.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File 2: The Image Predictions\n",
    "\n",
    "\n",
    "Now, we have to do a little more legwork. We can remotely 'boop' the Udacity servers using the Request library to download our second data source. From there, it's just a matter making sure the text is read properly, that Pandas recognizes the tab separated data format, and voila, we'll have our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2075 entries, 0 to 2074\n",
      "Data columns (total 12 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   tweet_id  2075 non-null   int64  \n",
      " 1   jpg_url   2075 non-null   object \n",
      " 2   img_num   2075 non-null   int64  \n",
      " 3   p1        2075 non-null   object \n",
      " 4   p1_conf   2075 non-null   float64\n",
      " 5   p1_dog    2075 non-null   bool   \n",
      " 6   p2        2075 non-null   object \n",
      " 7   p2_conf   2075 non-null   float64\n",
      " 8   p2_dog    2075 non-null   bool   \n",
      " 9   p3        2075 non-null   object \n",
      " 10  p3_conf   2075 non-null   float64\n",
      " 11  p3_dog    2075 non-null   bool   \n",
      "dtypes: bool(3), float64(3), int64(2), object(4)\n",
      "memory usage: 152.1+ KB\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import io\n",
    "\n",
    "# pull the data down from the Udacity servers\n",
    "r = requests.get('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv')\n",
    "\n",
    "# Use the encoded raw text, identify the separator to pandas, and load the dataframe,\n",
    "# preds here is shorthand for 'predictions'\n",
    "with open('image_predictions.tsv', mode = 'wb') as file:\n",
    "    file.write(r.content)\n",
    "\n",
    "predictions = pd.read_csv('image_predictions.tsv', sep='\\t')\n",
    "predictions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File 3: Additonal Twitter Details\n",
    "\n",
    "\n",
    "Here's where things get a little more... ruff. We're going to have to go ahead and use the `tweepy` library to programatically access twitter for all the additional information about the tweets we were interested in from above. \n",
    "\n",
    "\n",
    "Which tweets should we grab? Well we have the ids from the Twitter archive above. While this goes back further than we need, we can pare down our results later, once we have all the possible information assembled. For now, let's grab those tweets from the `dogs` df above, write the JSON we get back from the api into a file labeled `tweet_json.txt`, and read that into a Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 435\n",
      "Rate limit reached. Sleeping for: 656\n",
      "Rate limit reached. Sleeping for: 664\n"
     ]
    }
   ],
   "source": [
    "# grab the twitter data for each tweet and store it as a json array in \"tweet_json.txt\"\n",
    "import tweepy\n",
    "import json\n",
    "\n",
    "# Remember to load your own API key information from the Twitter Developers portal.\n",
    "api_key = ''\n",
    "api_secret = ''\n",
    "access_token = ''\n",
    "access_token_secret = ''\n",
    "\n",
    "# Set up our tweepy scraper, ensuring that we can use the right amount of request and not\n",
    "# violate our rate limit:\n",
    "auth = tweepy.OAuthHandler(api_key, api_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "# Set up an array to catch all the tweets that have since been deleted off Twitter\n",
    "deleted_tweets = []\n",
    "\n",
    "# Grab the IDs we need\n",
    "tweet_ids = df['tweet_id']\n",
    "# create our file if it doesn't exist\n",
    "with open('tweet_json.txt', 'w+') as f:\n",
    "    for tweet_id in tweet_ids:\n",
    "        # set up an exception handler in the event that we run into a tweet that no longer\n",
    "        # exists, and append that tweets ID into our array above\n",
    "        try:\n",
    "            # print('Grabbing tweet_id {}'.format(tweet_id))\n",
    "            tweet = api.get_status(tweet_id, tweet_mode='extended')\n",
    "            json.dump(tweet._json, f)\n",
    "            # write a newline to ensure that each tweet gets its own separate line in the text file:\n",
    "            f.write('\\n')\n",
    "        # document errors when they occur:\n",
    "        except tweepy.TweepError:\n",
    "            # print('Tweet not found: {}'.format(tweet_id))\n",
    "            deleted_tweets.append(tweet_id)\n",
    "            pass\n",
    "\n",
    "# create a blank dataframe to load our JSON data into:\n",
    "cols = ['tweet_id', 'retweet_count', 'favorite_count']\n",
    "api_dl = pd.DataFrame(columns=cols)\n",
    "with open('tweet_json.txt') as json_file:\n",
    "    for line in json_file:\n",
    "        # read through the file line by line, using the keys in the JSON file\n",
    "        # to pull out the appropriate data we need for our dataframe\n",
    "        status = json.loads(line)\n",
    "        tweet_id = status['id_str']\n",
    "        retweet_count = status['retweet_count']\n",
    "        favorite_count = status['favorite_count']\n",
    "        \n",
    "        # append each line to our dataframe\n",
    "        api_dl = api_data.append(pd.DataFrame([[tweet_id, retweet_count, favorite_count]], columns=cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892420643555336193</td>\n",
       "      <td>7724</td>\n",
       "      <td>36264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id  retweet_count  favorite_count\n",
       "0  892420643555336193           7724           36264"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm our data is loaded as expected\n",
    "api_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Assessing / Cleaning Our Data\n",
    "\n",
    "\n",
    "While normally, you can iterate through the data wrangling process by first identifying your issues as a whole, then cleaning them, for simplicity's sake, I will be identifying the issues with my data and cleaning them as I go. These issues will come in two main forms: data quality, and data tidiness. While data quality lacks a neat and *tidy* definition, tidiness does. In particular, [as defined by Hadley Wickham](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html), the three major characteristics of tidy data are:\n",
    "- Each variable forms a column\n",
    "- Each observation forms a row\n",
    "- Each observational unit forms a table\n",
    "\n",
    "\n",
    "Working backwards, we can identify data quality issues as those that deal with the actual data itself. Missing values, improper data types, improperly enterted values all enter the realm of \"quality\" rather than tidy. \n",
    "\n",
    "\n",
    "Bearing these definitions in mind, I'll first lay out an outline of issues I plan to address, and then we can work down the list one by one, expounding on them in more detail and addressing the issues as they occur.\n",
    "\n",
    "\n",
    "##### Data Quality Issues\n",
    "- Twitter Archive Table\n",
    "    1. Retweets exist in the table\n",
    "    3. Extra retweet columns exist in the table\n",
    "    4. Ratings have incorrect denominators\n",
    "    5. IDs are not all listed as strings\n",
    "    6. Dogs named 'None' and 'a'\n",
    "- Image Predictions Table\n",
    "    7. Prediction name columns are inconsistenly formatted.\n",
    "- Twitter API Data Table\n",
    "    8. Retweets and favorites should be listed as floats\n",
    "\n",
    "\n",
    "##### Data Tidiness Issues\n",
    "- Twitter Archive Table\n",
    "    1. Dog types are listed as separate columns\n",
    "- All Tables\n",
    "    2. Tables can be merged to provide related information\n",
    "\n",
    "However, firstly, before any of these can be addressed, we need to make copies of our data in case we need to revert back to the data previously loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogs = archive.copy()\n",
    "preds = predictions.copy()\n",
    "api_data = api_dl.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Quality Issues: Twitter Archive Table\n",
    "#### 1. Retweets exist in the table\n",
    "\n",
    "##### Define:\n",
    "Unfortunately not all the information we have concerns the pupperinos that We Rate Dogs has examined. According to the [Tweet object API](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object), the `retweeted_status_id` field will return a value in the event that the user has retweeted the post. Here we can take a look at the number of retweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall tweets: 2356\n",
      "Retweets: 181\n"
     ]
    }
   ],
   "source": [
    "# pull down the number of retweets in the dataset\n",
    "tweets = dogs.shape[0]\n",
    "print('Overall tweets:', dogs.shape[0])\n",
    "print('Retweets:', dogs[dogs['retweeted_status_id'].notnull()].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code:\n",
    "Now we clean out those retweets, and verify that there are no more, and that the shape of the dataframe has now dropped all of those rose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the dog retweets and check the new retweet counts\n",
    "dogs = dogs[dogs['retweeted_status_id'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test:\n",
    "Taking a look back at the same metrics we used before, we can see that all retweets have been dropped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall tweets: 2175\n",
      "Retweets: 0\n"
     ]
    }
   ],
   "source": [
    "print('Overall tweets:', dogs.shape[0])\n",
    "print('Retweets:', dogs[dogs['retweeted_status_id'].notnull()].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Sorted. Let's tackle the second issue.\n",
    "\n",
    "\n",
    "#### 2. Extra Retweet Data Exists\n",
    "\n",
    "##### Define\n",
    "Now that we've gotten rid of all the actual retweets, this renders columns that used to hold information about those retweets entirely unnessecary. While this does change the strucutre of the table, it still reflect a quality rather than a tidiness issue, as it doesn't reflect an adjustment to one of the core principles of tidy data listed above.\n",
    "\n",
    "\n",
    "We can take a look at the columns of the dataframe and identify which of those columns are no longer necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'in_reply_to_status_id', 'in_reply_to_user_id', 'timestamp',\n",
       "       'source', 'text', 'retweeted_status_id', 'retweeted_status_user_id',\n",
       "       'retweeted_status_timestamp', 'expanded_urls', 'rating_numerator',\n",
       "       'rating_denominator', 'name', 'doggo', 'floofer', 'pupper', 'puppo'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dogs.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code\n",
    "So it seems in particular that `retweeted_status_id`, which we just used to filter out the retweets, along with `retweeted_status_user_id`, and `retweeted_status_timestamp`, won't be necessary, as they reflect properties of the retweeted content, which we just filtered out. So we can first verify that there is no longer any data in these columns, drop them, and keep cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: retweeted_status_id, dtype: int64)\n",
      "Series([], Name: retweeted_status_timestamp, dtype: int64)\n",
      "Series([], Name: retweeted_status_user_id, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "print(dogs.retweeted_status_id.value_counts())\n",
    "print(dogs.retweeted_status_timestamp.value_counts())\n",
    "print(dogs.retweeted_status_user_id.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because all the series returned were empty, we've verified they've got no values, and can drop them accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogs.drop(columns=['retweeted_status_id', 'retweeted_status_timestamp', 'retweeted_status_user_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test\n",
    "A simple look at the columns should reveal they're all gone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'in_reply_to_status_id', 'in_reply_to_user_id', 'timestamp',\n",
       "       'source', 'text', 'expanded_urls', 'rating_numerator',\n",
       "       'rating_denominator', 'name', 'doggo', 'floofer', 'pupper', 'puppo'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dogs.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Ratings have incorrect denominators\n",
    "\n",
    "##### Define\n",
    "Taking a look at the unique values for the denominator, we can see values other than ten. We know from the documentation (as well as our genuine love for the WeRateDogs account) that the denominator is always 10, so we should correct for these mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10     2153\n",
       "50        3\n",
       "80        2\n",
       "11        2\n",
       "20        2\n",
       "2         1\n",
       "16        1\n",
       "40        1\n",
       "70        1\n",
       "15        1\n",
       "90        1\n",
       "110       1\n",
       "120       1\n",
       "130       1\n",
       "150       1\n",
       "170       1\n",
       "7         1\n",
       "0         1\n",
       "Name: rating_denominator, dtype: int64"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dogs['rating_denominator'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code\n",
    "Simply hard coding the denominator to what we know it should be will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    2175\n",
       "Name: rating_denominator, dtype: int64"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dogs['rating_denominator'] = 10\n",
    "dogs['rating_denominator'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test\n",
    "Running another value_counts() will show us that the only remaining value left is 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. IDs are listed as numbers rather than strings\n",
    "\n",
    "##### Define\n",
    "We won't actually be conducting any analysis on the values of the IDs of the tweets, and we certainly can't preform any mathematical operations on their values either. As a result, all IDs should be represented as strings, rather than integer values. We can fix this easily in the `dogs` dataframe, and whie I'm jumping ahead a bit, it's easy enough to correct for in the `preds` dataframe as well (I know it to not be an issue in the `api_data` dataframe as I assembled that one myself). Let's check out the values of these ID columns and fix 'em up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "print(dogs.dtypes['tweet_id'])\n",
    "print(preds.dtypes['tweet_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code\n",
    "Recasting the columns as strings will effectively solve the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogs['tweet_id'] = dogs['tweet_id'].astype(str)\n",
    "preds['tweet_id'] = preds['tweet_id'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test\n",
    "Checking the data types again should show the problem solved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "# Test our results:\n",
    "print(dogs.dtypes['tweet_id'])\n",
    "print(preds.dtypes['tweet_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Dogs named 'None' and 'a'\n",
    "\n",
    "\n",
    "##### Define\n",
    "While goodbois can have any name, some of the names are not formatted correctly in the `twitter_archive` file. While we can't extract these, we can at least get rid of the erroneous values so they're excluded from any of the name-related analysis we might conduct later on! We can do this by replacing the values we don't want with NaN, as it can be excluded from many analyses easily, and won't cause us to be confused by the 680 people who supposedly named their dog 'None'. Not what I would have gone for, but you do you I guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None       680\n",
       "a           55\n",
       "Lucy        11\n",
       "Charlie     11\n",
       "Cooper      10\n",
       "          ... \n",
       "Brudge       1\n",
       "Brutus       1\n",
       "Lizzie       1\n",
       "Grizzie      1\n",
       "Devón        1\n",
       "Name: name, Length: 956, dtype: int64"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we can identify two easy fixes, 'None' and 'a':\n",
    "dogs['name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code\n",
    "Replacing all locations where we find 'None' or 'a' with a NaN will allow these values to not be included in future analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lucy       11\n",
       "Charlie    11\n",
       "Oliver     10\n",
       "Cooper     10\n",
       "Tucker      9\n",
       "           ..\n",
       "Chelsea     1\n",
       "Brudge      1\n",
       "Brutus      1\n",
       "Lizzie      1\n",
       "Devón       1\n",
       "Name: name, Length: 954, dtype: int64"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace values with an acutal NaN value so we can't see them\n",
    "dogs.loc[(dogs.name==\"None\")|(dogs.name==\"a\"), 'name'] = np.nan\n",
    "dogs['name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Quality Issues: Image Predictions Table\n",
    "#### 7. Prediction name columns don't represent their contents.\n",
    "\n",
    "\n",
    "##### Define\n",
    "A simple enough one to fix. Taking a look at the predictions table, if you had never seen it before, it would be hard to identify what the contents of the columns was based on their headers alone. Fixing these with more descriptive names will make analyzing the data easier. For instance 'p1', 'p2', and 'p3' don't actually offer the proper information about what their columns, the predcition confidence for the first image prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'jpg_url', 'img_num', 'p1', 'p1_conf', 'p1_dog', 'p2',\n",
       "       'p2_conf', 'p2_dog', 'p3', 'p3_conf', 'p3_dog'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code\n",
    "We can simply create a dictionary of the values we want to replace, pass it to the .rename function, and be on our merry way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.rename(columns={'p1': 'image_prediction1',\n",
    "                      'p2': 'image_prediction2',\n",
    "                      'p3': 'image_prediction3',\n",
    "                      'p1_conf': 'prediction1_confidence',\n",
    "                      'p2_conf': 'prediction2_confidence',\n",
    "                      'p3_conf': 'prediction3_confidence',\n",
    "                      'p1_dog': 'is_prediction1_dog',\n",
    "                      'p2_dog': 'is_prediction2_dog',\n",
    "                      'p3_dog': 'is_prediction3_dog'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test\n",
    "With the mapping complete, our columns variable should now reflect the updated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'jpg_url', 'img_num', 'image_prediction1',\n",
       "       'prediction1_confidence', 'is_prediction1_dog', 'image_prediction2',\n",
       "       'prediction2_confidence', 'is_prediction2_dog', 'image_prediction3',\n",
       "       'prediction3_confidence', 'is_prediction3_dog'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Quality Issues: Twitter Data Table\n",
    "#### 8. Retweets and Favorites Represented as Strings\n",
    "\n",
    "##### Define\n",
    "While I could have fixed this in the process of wrangling my data, it is still an issue with the data set as it currently exists. As of right now, all the data in this `api_data` dataframe is stored as a string. As we addressed above, this is fine for the `tweet_id` field, as we won't be preforming any numerical analysis on the ID values. However, as we will be doing that for both the `retweet_count` and `favorite_count` fields, we'll need to make an adjustment. In this case, it makes the most sense to use integer, as we're interested only in whole number values for each of these variables. While it might enable more precision for averages, you can't ultimately wind up with half a retweet or a favorite, only whole number amounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2 entries, 0 to 0\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   tweet_id        2 non-null      object\n",
      " 1   retweet_count   2 non-null      int64 \n",
      " 2   favorite_count  2 non-null      int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 64.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# verify that the columns are strings (object representation)\n",
    "api_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code\n",
    "Recasting the columns as we did above for strings should suffice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the columns to integers\n",
    "api_data[['retweet_count', 'favorite_count']] = api_data[['retweet_count', 'favorite_count']].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test\n",
    "Rechecking the data types will show us the conversion was successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2 entries, 0 to 0\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   tweet_id        2 non-null      object\n",
      " 1   retweet_count   2 non-null      int64 \n",
      " 2   favorite_count  2 non-null      int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 64.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "api_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With several of the glaring data quality issues out of the way, now we can address the overall structure of our files, and in turn, their respective tidiness (or lack thereof).\n",
    "\n",
    "#### Data Tidiness Issues\n",
    "#### 1. Dog types represented across columns\n",
    "##### Define\n",
    "Back in the `dogs` dataframe (the `twitter-archive` dataframe), if we take a look, we can see that information about the dog types included spans across multiple columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doggo</th>\n",
       "      <th>floofer</th>\n",
       "      <th>pupper</th>\n",
       "      <th>puppo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doggo floofer pupper puppo\n",
       "0  None    None   None  None\n",
       "1  None    None   None  None\n",
       "2  None    None   None  None\n",
       "3  None    None   None  None\n",
       "4  None    None   None  None"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dogs[['doggo', 'floofer', 'pupper', 'puppo']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we can handle this data by merging these pieces of information into a new column we can call `dog_stage` which represents the values of all of these columns combined. Doing this is a little more involved, but will allow this particular dataframe to comply by a standard of tidy data, namely that \"each variable forms a column\". So let's do that!\n",
    "\n",
    "\n",
    "##### Code\n",
    "First, we'll take a look and see if we need to do any additional transformation to the values in the columns themselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None     2088\n",
      "doggo      87\n",
      "Name: doggo, dtype: int64\n",
      "None       2165\n",
      "floofer      10\n",
      "Name: floofer, dtype: int64\n",
      "None      1941\n",
      "pupper     234\n",
      "Name: pupper, dtype: int64\n",
      "None     2150\n",
      "puppo      25\n",
      "Name: puppo, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dogs.doggo.value_counts())\n",
    "print(dogs.floofer.value_counts())\n",
    "print(dogs.pupper.value_counts())\n",
    "print(dogs.puppo.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it looks as though there's only a \"None\" value, or the string that represents the column itself (e.g. 'doggo', 'floofer', etc). It seems like the thing to do here is to stitch these columns together, and replace the instances we find where multiple matches occur in the same rows. In order to do that, we can replace the 'None' values with an empty string, concatenate all the columns together, and adjust for the overlapping examples we find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None values with empty strings to concatenate\n",
    "for col in ['doggo', 'floofer', 'pupper', 'puppo']:\n",
    "    dogs.loc[dogs[col]==\"None\", col] = ''\n",
    "\n",
    "# construct a column which concatenates all four columns together\n",
    "dogs['dog_stages'] = dogs['doggo'] + dogs['floofer'] + dogs['pupper'] + dogs['puppo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                1831\n",
       "pupper           224\n",
       "doggo             75\n",
       "puppo             24\n",
       "doggopupper       10\n",
       "floofer            9\n",
       "doggopuppo         1\n",
       "doggofloofer       1\n",
       "Name: dog_stages, dtype: int64"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review overlapping columns to correct with a dictionary\n",
    "dogs.dog_stages.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values with dictionary\n",
    "dogs.dog_stages.replace({'doggopupper': 'doggo, pupper',\n",
    "                         'doggopuppo': 'doggo, puppo',\n",
    "                         'doggofloofer': 'doggo, floofer'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test\n",
    "\n",
    "Looking at the value counts should confirm those values have been replace, the columns have been merged properly, we can drop the no longer needed columns and can move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  1831\n",
       "pupper             224\n",
       "doggo               75\n",
       "puppo               24\n",
       "doggo, pupper       10\n",
       "floofer              9\n",
       "doggo, floofer       1\n",
       "doggo, puppo         1\n",
       "Name: dog_stages, dtype: int64"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that all the values have been fixed\n",
    "dogs.dog_stages.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the columns have been merged properly, we can drop the no longer needed columns and move on to the next issue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "dogs.drop(['doggo', 'floofer', 'pupper', 'puppo'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Tables can be merged to provide related information\n",
    "##### Define\n",
    "Two of the rules of tidy data observed above are:\n",
    "- Each observation forms a row\n",
    "- Each type of observational unit forms a table\n",
    "\n",
    "\n",
    "In this case, we've got three tables each containing data regarding individual tweets. While they all contain separate information about those tweets, the observational units are all the same, and as such, according to the rules of tidy data they should be merged for the best analysis. So, we can link all three dataframes together using the `tweet_id` field, which all three data frames have, and export our data, clean and ready for new analyses!\n",
    "\n",
    "\n",
    "However, one thing we do have to be aware of is that we don't have perfectly overlapping datasets, so in this case, if we want to ensure our data stays clean, we'll have to conduct an 'inner merge', rather than a left or right join, effectively making our final compiled dataset the intersection of the three datasets, where each `tweet_id` is present in all three.\n",
    "\n",
    "\n",
    "Then we can write our file to a master .csv file which we can then use to preform any analysis on going forward.\n",
    "\n",
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the three datasets together using the formatted tweet_id fields from before\n",
    "twitter_archive_master = dogs.merge(preds, on='tweet_id').merge(api_data, on='tweet_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test\n",
    "Looking at the information for the new dataframe we should see the columns from all the dataframes above. I'll also use this cell to write the dataframe to the output it needs to take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2 entries, 0 to 1\n",
      "Data columns (total 24 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   tweet_id                2 non-null      object \n",
      " 1   in_reply_to_status_id   0 non-null      float64\n",
      " 2   in_reply_to_user_id     0 non-null      float64\n",
      " 3   timestamp               2 non-null      object \n",
      " 4   source                  2 non-null      object \n",
      " 5   text                    2 non-null      object \n",
      " 6   expanded_urls           2 non-null      object \n",
      " 7   rating_numerator        2 non-null      int64  \n",
      " 8   rating_denominator      2 non-null      int64  \n",
      " 9   name                    2 non-null      object \n",
      " 10  dog_stages              2 non-null      object \n",
      " 11  jpg_url                 2 non-null      object \n",
      " 12  img_num                 2 non-null      int64  \n",
      " 13  image_prediction1       2 non-null      object \n",
      " 14  prediction1_confidence  2 non-null      float64\n",
      " 15  is_prediction1_dog      2 non-null      bool   \n",
      " 16  image_prediction2       2 non-null      object \n",
      " 17  prediction2_confidence  2 non-null      float64\n",
      " 18  is_prediction2_dog      2 non-null      bool   \n",
      " 19  image_prediction3       2 non-null      object \n",
      " 20  prediction3_confidence  2 non-null      float64\n",
      " 21  is_prediction3_dog      2 non-null      bool   \n",
      " 22  retweet_count           2 non-null      int64  \n",
      " 23  favorite_count          2 non-null      int64  \n",
      "dtypes: bool(3), float64(5), int64(5), object(11)\n",
      "memory usage: 358.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "twitter_archive_master.to_csv('twitter_master_archive.csv', index=False)\n",
    "twitter_archive_master.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "While every single issue with the dataset hasn't been addressed here, it certainly leaves us with a clean enough data file that we can start to do some real work on, and start to identify what separates the good bois from the best bois! Happy analyzing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
